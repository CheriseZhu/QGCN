INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
INFO:root:Num classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=16, c=tensor([-1.], device='cuda:0', grad_fn=<CopyBackwards>))
        (agg): HypAgg(
          c=tensor([-1.], device='cuda:0', grad_fn=<CopyBackwards>)
          (att): DenseAtt(
            (linear): Linear(in_features=32, out_features=1, bias=True)
          )
        )
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<CopyBackwards>), c_out=tensor([-1.], device='cuda:0', grad_fn=<CopyBackwards>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=16, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<CopyBackwards>)
    (cls): Linear(
      (linear): Linear(in_features=16, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 59415
/workspace/anaconda3/envs/geo/lib/python3.7/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/workspace/anaconda3/envs/geo/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:370: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  "please use `get_last_lr()`.", UserWarning)
INFO:root:Epoch: 0005 lr: 0.01 train_loss: 1.7915 train_acc: 0.1583 train_f1: 0.1583 time: 0.4708s
INFO:root:Epoch: 0005 val_loss: 1.8135 val_acc: 0.0580 val_f1: 0.0580
INFO:root:Epoch: 0010 lr: 0.01 train_loss: 1.7965 train_acc: 0.1167 train_f1: 0.1167 time: 0.4766s
INFO:root:Epoch: 0010 val_loss: 1.7955 val_acc: 0.2320 val_f1: 0.2320
INFO:root:Epoch: 0015 lr: 0.01 train_loss: 1.7678 train_acc: 0.2250 train_f1: 0.2250 time: 0.4762s
INFO:root:Epoch: 0015 val_loss: 1.7796 val_acc: 0.2360 val_f1: 0.2360
INFO:root:Epoch: 0020 lr: 0.01 train_loss: 1.7410 train_acc: 0.4000 train_f1: 0.4000 time: 0.4708s
INFO:root:Epoch: 0020 val_loss: 1.7713 val_acc: 0.2520 val_f1: 0.2520
INFO:root:Epoch: 0025 lr: 0.01 train_loss: 1.7116 train_acc: 0.3333 train_f1: 0.3333 time: 0.4640s
INFO:root:Epoch: 0025 val_loss: 1.7659 val_acc: 0.1520 val_f1: 0.1520
INFO:root:Epoch: 0030 lr: 0.01 train_loss: 1.6825 train_acc: 0.4000 train_f1: 0.4000 time: 0.4746s
INFO:root:Epoch: 0030 val_loss: 1.7512 val_acc: 0.1980 val_f1: 0.1980
INFO:root:Epoch: 0035 lr: 0.01 train_loss: 1.6292 train_acc: 0.4583 train_f1: 0.4583 time: 0.4629s
INFO:root:Epoch: 0035 val_loss: 1.7271 val_acc: 0.3020 val_f1: 0.3020
INFO:root:Epoch: 0040 lr: 0.01 train_loss: 1.5710 train_acc: 0.5083 train_f1: 0.5083 time: 0.4619s
INFO:root:Epoch: 0040 val_loss: 1.6895 val_acc: 0.4700 val_f1: 0.4700
INFO:root:Epoch: 0045 lr: 0.01 train_loss: 1.4397 train_acc: 0.5500 train_f1: 0.5500 time: 0.4588s
INFO:root:Epoch: 0045 val_loss: 1.6484 val_acc: 0.5320 val_f1: 0.5320
INFO:root:Epoch: 0050 lr: 0.01 train_loss: 1.3406 train_acc: 0.6000 train_f1: 0.6000 time: 0.4733s
INFO:root:Epoch: 0050 val_loss: 1.6092 val_acc: 0.5320 val_f1: 0.5320
INFO:root:Epoch: 0055 lr: 0.01 train_loss: 1.2834 train_acc: 0.5750 train_f1: 0.5750 time: 0.4652s
INFO:root:Epoch: 0055 val_loss: 1.5654 val_acc: 0.5720 val_f1: 0.5720
INFO:root:Epoch: 0060 lr: 0.01 train_loss: 1.2232 train_acc: 0.5750 train_f1: 0.5750 time: 0.4687s
INFO:root:Epoch: 0060 val_loss: 1.5144 val_acc: 0.6260 val_f1: 0.6260
INFO:root:Epoch: 0065 lr: 0.01 train_loss: 1.1932 train_acc: 0.5583 train_f1: 0.5583 time: 0.4696s
INFO:root:Epoch: 0065 val_loss: 1.4662 val_acc: 0.6520 val_f1: 0.6520
INFO:root:Epoch: 0070 lr: 0.01 train_loss: 1.2224 train_acc: 0.4917 train_f1: 0.4917 time: 0.4584s
INFO:root:Epoch: 0070 val_loss: 1.4307 val_acc: 0.6360 val_f1: 0.6360
INFO:root:Epoch: 0075 lr: 0.01 train_loss: 1.1661 train_acc: 0.4750 train_f1: 0.4750 time: 0.4599s
INFO:root:Epoch: 0075 val_loss: 1.3976 val_acc: 0.6540 val_f1: 0.6540
INFO:root:Epoch: 0080 lr: 0.01 train_loss: 1.0354 train_acc: 0.5750 train_f1: 0.5750 time: 0.4922s
INFO:root:Epoch: 0080 val_loss: 1.3633 val_acc: 0.6640 val_f1: 0.6640
INFO:root:Epoch: 0085 lr: 0.01 train_loss: 0.9610 train_acc: 0.5583 train_f1: 0.5583 time: 0.4552s
INFO:root:Epoch: 0085 val_loss: 1.3450 val_acc: 0.6360 val_f1: 0.6360
INFO:root:Epoch: 0090 lr: 0.01 train_loss: 0.9915 train_acc: 0.6250 train_f1: 0.6250 time: 0.4678s
INFO:root:Epoch: 0090 val_loss: 1.3186 val_acc: 0.6260 val_f1: 0.6260
INFO:root:Epoch: 0095 lr: 0.01 train_loss: 1.0223 train_acc: 0.5667 train_f1: 0.5667 time: 0.4773s
INFO:root:Epoch: 0095 val_loss: 1.2773 val_acc: 0.6520 val_f1: 0.6520
INFO:root:Epoch: 0100 lr: 0.01 train_loss: 0.7873 train_acc: 0.6917 train_f1: 0.6917 time: 0.4713s
INFO:root:Epoch: 0100 val_loss: 1.2490 val_acc: 0.6560 val_f1: 0.6560
INFO:root:Epoch: 0105 lr: 0.01 train_loss: 0.8402 train_acc: 0.6333 train_f1: 0.6333 time: 0.4609s
INFO:root:Epoch: 0105 val_loss: 1.2367 val_acc: 0.6480 val_f1: 0.6480
INFO:root:Epoch: 0110 lr: 0.01 train_loss: 0.8391 train_acc: 0.6417 train_f1: 0.6417 time: 0.4630s
INFO:root:Epoch: 0110 val_loss: 1.2371 val_acc: 0.6340 val_f1: 0.6340
INFO:root:Epoch: 0115 lr: 0.01 train_loss: 0.8103 train_acc: 0.6083 train_f1: 0.6083 time: 0.4783s
INFO:root:Epoch: 0115 val_loss: 1.2257 val_acc: 0.6300 val_f1: 0.6300
INFO:root:Epoch: 0120 lr: 0.01 train_loss: 0.7472 train_acc: 0.6667 train_f1: 0.6667 time: 0.4720s
INFO:root:Epoch: 0120 val_loss: 1.2014 val_acc: 0.6460 val_f1: 0.6460
INFO:root:Epoch: 0125 lr: 0.01 train_loss: 0.8316 train_acc: 0.6417 train_f1: 0.6417 time: 0.4737s
INFO:root:Epoch: 0125 val_loss: 1.1802 val_acc: 0.6500 val_f1: 0.6500
INFO:root:Epoch: 0130 lr: 0.01 train_loss: 0.7154 train_acc: 0.7333 train_f1: 0.7333 time: 0.4768s
INFO:root:Epoch: 0130 val_loss: 1.1604 val_acc: 0.6420 val_f1: 0.6420
INFO:root:Epoch: 0135 lr: 0.01 train_loss: 0.7174 train_acc: 0.7167 train_f1: 0.7167 time: 0.4708s
INFO:root:Epoch: 0135 val_loss: 1.1549 val_acc: 0.6500 val_f1: 0.6500
INFO:root:Epoch: 0140 lr: 0.01 train_loss: 0.7130 train_acc: 0.6667 train_f1: 0.6667 time: 0.4740s
INFO:root:Epoch: 0140 val_loss: 1.1745 val_acc: 0.6400 val_f1: 0.6400
INFO:root:Epoch: 0145 lr: 0.01 train_loss: 0.7387 train_acc: 0.6833 train_f1: 0.6833 time: 0.4709s
INFO:root:Epoch: 0145 val_loss: 1.1579 val_acc: 0.6440 val_f1: 0.6440
INFO:root:Epoch: 0150 lr: 0.01 train_loss: 0.7302 train_acc: 0.7250 train_f1: 0.7250 time: 0.5116s
INFO:root:Epoch: 0150 val_loss: 1.1188 val_acc: 0.6600 val_f1: 0.6600
INFO:root:Epoch: 0155 lr: 0.01 train_loss: 0.7078 train_acc: 0.7083 train_f1: 0.7083 time: 0.4695s
INFO:root:Epoch: 0155 val_loss: 1.1130 val_acc: 0.6540 val_f1: 0.6540
INFO:root:Epoch: 0160 lr: 0.01 train_loss: 0.7918 train_acc: 0.6000 train_f1: 0.6000 time: 0.4779s
INFO:root:Epoch: 0160 val_loss: 1.1124 val_acc: 0.6460 val_f1: 0.6460
INFO:root:Epoch: 0165 lr: 0.01 train_loss: 0.7641 train_acc: 0.6333 train_f1: 0.6333 time: 0.4716s
INFO:root:Epoch: 0165 val_loss: 1.1169 val_acc: 0.6420 val_f1: 0.6420
INFO:root:Epoch: 0170 lr: 0.01 train_loss: 0.7469 train_acc: 0.6083 train_f1: 0.6083 time: 0.4577s
INFO:root:Epoch: 0170 val_loss: 1.1125 val_acc: 0.6440 val_f1: 0.6440
INFO:root:Epoch: 0175 lr: 0.01 train_loss: 0.5952 train_acc: 0.7000 train_f1: 0.7000 time: 0.4758s
INFO:root:Epoch: 0175 val_loss: 1.1000 val_acc: 0.6480 val_f1: 0.6480
INFO:root:Epoch: 0180 lr: 0.01 train_loss: 0.6898 train_acc: 0.6583 train_f1: 0.6583 time: 0.4675s
INFO:root:Epoch: 0180 val_loss: 1.0903 val_acc: 0.6540 val_f1: 0.6540
INFO:root:Early stopping
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 136.9628s
INFO:root:Val set results: val_loss: 1.3633 val_acc: 0.6640 val_f1: 0.6640
INFO:root:Test set results: test_loss: 1.3567 test_acc: 0.6750 test_f1: 0.6750
INFO:root:Saved model in /workspace/xiongbo/hgcn/logs/nc/2021_4_22/29
