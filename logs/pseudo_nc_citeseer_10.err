INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
INFO:root:Num classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=16, c=tensor([-1.], device='cuda:0', grad_fn=<CopyBackwards>))
        (agg): HypAgg(
          c=tensor([-1.], device='cuda:0', grad_fn=<CopyBackwards>)
          (att): DenseAtt(
            (linear): Linear(in_features=32, out_features=1, bias=True)
          )
        )
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<CopyBackwards>), c_out=tensor([-1.], device='cuda:0', grad_fn=<CopyBackwards>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=16, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<CopyBackwards>)
    (cls): Linear(
      (linear): Linear(in_features=16, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 59415
/workspace/anaconda3/envs/geo/lib/python3.7/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/workspace/anaconda3/envs/geo/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:370: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  "please use `get_last_lr()`.", UserWarning)
INFO:root:Epoch: 0005 lr: 0.01 train_loss: 1.7908 train_acc: 0.1583 train_f1: 0.1583 time: 0.4825s
INFO:root:Epoch: 0005 val_loss: 1.8126 val_acc: 0.0580 val_f1: 0.0580
INFO:root:Epoch: 0010 lr: 0.01 train_loss: 1.7956 train_acc: 0.1167 train_f1: 0.1167 time: 0.4093s
INFO:root:Epoch: 0010 val_loss: 1.7937 val_acc: 0.2320 val_f1: 0.2320
INFO:root:Epoch: 0015 lr: 0.01 train_loss: 1.7651 train_acc: 0.2833 train_f1: 0.2833 time: 0.4815s
INFO:root:Epoch: 0015 val_loss: 1.7776 val_acc: 0.3400 val_f1: 0.3400
INFO:root:Epoch: 0020 lr: 0.01 train_loss: 1.7354 train_acc: 0.3750 train_f1: 0.3750 time: 0.4843s
INFO:root:Epoch: 0020 val_loss: 1.7686 val_acc: 0.2040 val_f1: 0.2040
INFO:root:Epoch: 0025 lr: 0.01 train_loss: 1.7017 train_acc: 0.3250 train_f1: 0.3250 time: 0.4791s
INFO:root:Epoch: 0025 val_loss: 1.7620 val_acc: 0.1460 val_f1: 0.1460
INFO:root:Epoch: 0030 lr: 0.01 train_loss: 1.6691 train_acc: 0.4417 train_f1: 0.4417 time: 0.4726s
INFO:root:Epoch: 0030 val_loss: 1.7449 val_acc: 0.2100 val_f1: 0.2100
INFO:root:Epoch: 0035 lr: 0.01 train_loss: 1.6136 train_acc: 0.4583 train_f1: 0.4583 time: 0.4675s
INFO:root:Epoch: 0035 val_loss: 1.7189 val_acc: 0.3200 val_f1: 0.3200
INFO:root:Epoch: 0040 lr: 0.01 train_loss: 1.5545 train_acc: 0.5250 train_f1: 0.5250 time: 0.4805s
INFO:root:Epoch: 0040 val_loss: 1.6826 val_acc: 0.4480 val_f1: 0.4480
INFO:root:Epoch: 0045 lr: 0.01 train_loss: 1.4317 train_acc: 0.5417 train_f1: 0.5417 time: 0.4786s
INFO:root:Epoch: 0045 val_loss: 1.6438 val_acc: 0.5080 val_f1: 0.5080
INFO:root:Epoch: 0050 lr: 0.01 train_loss: 1.3355 train_acc: 0.6000 train_f1: 0.6000 time: 0.4702s
INFO:root:Epoch: 0050 val_loss: 1.6073 val_acc: 0.5240 val_f1: 0.5240
INFO:root:Epoch: 0055 lr: 0.01 train_loss: 1.2792 train_acc: 0.5917 train_f1: 0.5917 time: 0.4655s
INFO:root:Epoch: 0055 val_loss: 1.5660 val_acc: 0.5800 val_f1: 0.5800
INFO:root:Epoch: 0060 lr: 0.01 train_loss: 1.2203 train_acc: 0.5833 train_f1: 0.5833 time: 0.4767s
INFO:root:Epoch: 0060 val_loss: 1.5172 val_acc: 0.6160 val_f1: 0.6160
INFO:root:Epoch: 0065 lr: 0.01 train_loss: 1.1992 train_acc: 0.5667 train_f1: 0.5667 time: 0.4796s
INFO:root:Epoch: 0065 val_loss: 1.4711 val_acc: 0.6480 val_f1: 0.6480
INFO:root:Epoch: 0070 lr: 0.01 train_loss: 1.2225 train_acc: 0.4917 train_f1: 0.4917 time: 0.4654s
INFO:root:Epoch: 0070 val_loss: 1.4333 val_acc: 0.6500 val_f1: 0.6500
INFO:root:Epoch: 0075 lr: 0.01 train_loss: 1.1753 train_acc: 0.5000 train_f1: 0.5000 time: 0.4725s
INFO:root:Epoch: 0075 val_loss: 1.3999 val_acc: 0.6740 val_f1: 0.6740
INFO:root:Epoch: 0080 lr: 0.01 train_loss: 1.0528 train_acc: 0.5917 train_f1: 0.5917 time: 0.4698s
INFO:root:Epoch: 0080 val_loss: 1.3707 val_acc: 0.6640 val_f1: 0.6640
INFO:root:Epoch: 0085 lr: 0.01 train_loss: 0.9724 train_acc: 0.5583 train_f1: 0.5583 time: 0.4760s
INFO:root:Epoch: 0085 val_loss: 1.3523 val_acc: 0.6460 val_f1: 0.6460
INFO:root:Epoch: 0090 lr: 0.01 train_loss: 0.9980 train_acc: 0.6417 train_f1: 0.6417 time: 0.4832s
INFO:root:Epoch: 0090 val_loss: 1.3280 val_acc: 0.6420 val_f1: 0.6420
INFO:root:Epoch: 0095 lr: 0.01 train_loss: 1.0315 train_acc: 0.5917 train_f1: 0.5917 time: 0.5119s
INFO:root:Epoch: 0095 val_loss: 1.2940 val_acc: 0.6600 val_f1: 0.6600
INFO:root:Epoch: 0100 lr: 0.01 train_loss: 0.7978 train_acc: 0.6750 train_f1: 0.6750 time: 0.4671s
INFO:root:Epoch: 0100 val_loss: 1.2681 val_acc: 0.6580 val_f1: 0.6580
INFO:root:Epoch: 0105 lr: 0.01 train_loss: 0.8433 train_acc: 0.6250 train_f1: 0.6250 time: 0.4854s
INFO:root:Epoch: 0105 val_loss: 1.2504 val_acc: 0.6480 val_f1: 0.6480
INFO:root:Epoch: 0110 lr: 0.01 train_loss: 0.8483 train_acc: 0.6333 train_f1: 0.6333 time: 0.4843s
INFO:root:Epoch: 0110 val_loss: 1.2400 val_acc: 0.6480 val_f1: 0.6480
INFO:root:Epoch: 0115 lr: 0.01 train_loss: 0.8171 train_acc: 0.6083 train_f1: 0.6083 time: 0.4828s
INFO:root:Epoch: 0115 val_loss: 1.2196 val_acc: 0.6480 val_f1: 0.6480
INFO:root:Epoch: 0120 lr: 0.01 train_loss: 0.7460 train_acc: 0.6833 train_f1: 0.6833 time: 0.4884s
INFO:root:Epoch: 0120 val_loss: 1.2052 val_acc: 0.6500 val_f1: 0.6500
INFO:root:Epoch: 0125 lr: 0.01 train_loss: 0.8393 train_acc: 0.6417 train_f1: 0.6417 time: 0.4850s
INFO:root:Epoch: 0125 val_loss: 1.1902 val_acc: 0.6460 val_f1: 0.6460
INFO:root:Epoch: 0130 lr: 0.01 train_loss: 0.7129 train_acc: 0.7250 train_f1: 0.7250 time: 0.4650s
INFO:root:Epoch: 0130 val_loss: 1.1695 val_acc: 0.6500 val_f1: 0.6500
INFO:root:Epoch: 0135 lr: 0.01 train_loss: 0.7244 train_acc: 0.7167 train_f1: 0.7167 time: 0.4818s
INFO:root:Epoch: 0135 val_loss: 1.1528 val_acc: 0.6560 val_f1: 0.6560
INFO:root:Epoch: 0140 lr: 0.01 train_loss: 0.7134 train_acc: 0.6583 train_f1: 0.6583 time: 0.4790s
INFO:root:Epoch: 0140 val_loss: 1.1516 val_acc: 0.6460 val_f1: 0.6460
INFO:root:Epoch: 0145 lr: 0.01 train_loss: 0.7344 train_acc: 0.6833 train_f1: 0.6833 time: 0.4682s
INFO:root:Epoch: 0145 val_loss: 1.1485 val_acc: 0.6600 val_f1: 0.6600
INFO:root:Epoch: 0150 lr: 0.01 train_loss: 0.7241 train_acc: 0.7167 train_f1: 0.7167 time: 0.4726s
INFO:root:Epoch: 0150 val_loss: 1.1226 val_acc: 0.6560 val_f1: 0.6560
INFO:root:Epoch: 0155 lr: 0.01 train_loss: 0.7036 train_acc: 0.7083 train_f1: 0.7083 time: 0.4805s
INFO:root:Epoch: 0155 val_loss: 1.1099 val_acc: 0.6580 val_f1: 0.6580
INFO:root:Epoch: 0160 lr: 0.01 train_loss: 0.7887 train_acc: 0.6000 train_f1: 0.6000 time: 0.4851s
INFO:root:Epoch: 0160 val_loss: 1.1037 val_acc: 0.6520 val_f1: 0.6520
INFO:root:Epoch: 0165 lr: 0.01 train_loss: 0.7600 train_acc: 0.6333 train_f1: 0.6333 time: 0.4801s
INFO:root:Epoch: 0165 val_loss: 1.1011 val_acc: 0.6500 val_f1: 0.6500
INFO:root:Epoch: 0170 lr: 0.01 train_loss: 0.7406 train_acc: 0.6083 train_f1: 0.6083 time: 0.4796s
INFO:root:Epoch: 0170 val_loss: 1.0999 val_acc: 0.6500 val_f1: 0.6500
INFO:root:Epoch: 0175 lr: 0.01 train_loss: 0.5923 train_acc: 0.7000 train_f1: 0.7000 time: 0.4710s
INFO:root:Epoch: 0175 val_loss: 1.0884 val_acc: 0.6580 val_f1: 0.6580
INFO:root:Early stopping
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 135.0096s
INFO:root:Val set results: val_loss: 1.3999 val_acc: 0.6740 val_f1: 0.6740
INFO:root:Test set results: test_loss: 1.3974 test_acc: 0.6750 test_f1: 0.6750
INFO:root:Saved model in /workspace/xiongbo/hgcn/logs/nc/2021_4_22/31
