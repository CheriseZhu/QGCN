INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=16, c=tensor([-1.], device='cuda:0', grad_fn=<CopyBackwards>))
        (agg): HypAgg(
          c=tensor([-1.], device='cuda:0', grad_fn=<CopyBackwards>)
          (att): DenseAtt(
            (linear): Linear(in_features=32, out_features=1, bias=True)
          )
        )
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<CopyBackwards>), c_out=tensor([-1.], device='cuda:0', grad_fn=<CopyBackwards>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=16, out_features=16, c=tensor([-1.], device='cuda:0', grad_fn=<CopyBackwards>))
        (agg): HypAgg(
          c=tensor([-1.], device='cuda:0', grad_fn=<CopyBackwards>)
          (att): DenseAtt(
            (linear): Linear(in_features=32, out_features=1, bias=True)
          )
        )
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<CopyBackwards>), c_out=tensor([-1.], device='cuda:0', grad_fn=<CopyBackwards>))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 23298
/workspace/anaconda3/envs/geo/lib/python3.7/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/workspace/anaconda3/envs/geo/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:370: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  "please use `get_last_lr()`.", UserWarning)
INFO:root:Epoch: 0005 lr: 0.01 train_loss: 2.2536 train_roc: 0.9602 train_ap: 0.9606 time: 0.6356s
INFO:root:Epoch: 0005 val_loss: 2.2538 val_roc: 0.8268 val_ap: 0.8341
INFO:root:Epoch: 0010 lr: 0.01 train_loss: 2.2534 train_roc: 0.9679 train_ap: 0.9686 time: 0.6808s
INFO:root:Epoch: 0010 val_loss: 2.2535 val_roc: 0.8549 val_ap: 0.8580
INFO:root:Epoch: 0015 lr: 0.01 train_loss: 2.2520 train_roc: 0.9636 train_ap: 0.9648 time: 0.5823s
INFO:root:Epoch: 0015 val_loss: 2.2524 val_roc: 0.8583 val_ap: 0.8522
INFO:root:Epoch: 0020 lr: 0.01 train_loss: 2.2456 train_roc: 0.9619 train_ap: 0.9605 time: 0.6398s
INFO:root:Epoch: 0020 val_loss: 2.2476 val_roc: 0.8539 val_ap: 0.8437
INFO:root:Epoch: 0025 lr: 0.01 train_loss: 2.2280 train_roc: 0.9426 train_ap: 0.9395 time: 0.6380s
INFO:root:Epoch: 0025 val_loss: 2.2251 val_roc: 0.8351 val_ap: 0.8260
INFO:root:Epoch: 0030 lr: 0.01 train_loss: 2.1243 train_roc: 0.9116 train_ap: 0.9033 time: 0.6147s
INFO:root:Epoch: 0030 val_loss: 2.1379 val_roc: 0.7988 val_ap: 0.7800
INFO:root:Epoch: 0035 lr: 0.01 train_loss: 1.9755 train_roc: 0.9034 train_ap: 0.8901 time: 0.6197s
INFO:root:Epoch: 0035 val_loss: 1.9180 val_roc: 0.7723 val_ap: 0.7285
INFO:root:Epoch: 0040 lr: 0.01 train_loss: 1.2491 train_roc: 0.8042 train_ap: 0.7568 time: 0.5841s
INFO:root:Epoch: 0040 val_loss: 1.7649 val_roc: 0.6759 val_ap: 0.6843
INFO:root:Epoch: 0045 lr: 0.01 train_loss: 1.2866 train_roc: 0.7773 train_ap: 0.7207 time: 0.5914s
INFO:root:Epoch: 0045 val_loss: 1.6089 val_roc: 0.6731 val_ap: 0.6218
INFO:root:Epoch: 0050 lr: 0.01 train_loss: 1.2313 train_roc: 0.8139 train_ap: 0.7676 time: 0.5992s
INFO:root:Epoch: 0050 val_loss: 1.5555 val_roc: 0.7141 val_ap: 0.6752
INFO:root:Epoch: 0055 lr: 0.01 train_loss: 1.3051 train_roc: 0.7858 train_ap: 0.7472 time: 0.5869s
INFO:root:Epoch: 0055 val_loss: 1.6252 val_roc: 0.6908 val_ap: 0.6679
INFO:root:Epoch: 0060 lr: 0.01 train_loss: 1.2921 train_roc: 0.8451 train_ap: 0.8271 time: 0.6076s
INFO:root:Epoch: 0060 val_loss: 1.5230 val_roc: 0.7198 val_ap: 0.6809
INFO:root:Epoch: 0065 lr: 0.01 train_loss: 1.2249 train_roc: 0.8174 train_ap: 0.7689 time: 0.5834s
INFO:root:Epoch: 0065 val_loss: 1.4897 val_roc: 0.7148 val_ap: 0.6765
INFO:root:Epoch: 0070 lr: 0.01 train_loss: 1.6925 train_roc: 0.8704 train_ap: 0.8542 time: 0.5853s
INFO:root:Epoch: 0070 val_loss: 1.4721 val_roc: 0.7202 val_ap: 0.6826
INFO:root:Epoch: 0075 lr: 0.01 train_loss: 1.1739 train_roc: 0.8322 train_ap: 0.7870 time: 0.5895s
INFO:root:Epoch: 0075 val_loss: 1.4926 val_roc: 0.7242 val_ap: 0.6774
INFO:root:Epoch: 0080 lr: 0.01 train_loss: 1.1775 train_roc: 0.8445 train_ap: 0.8043 time: 0.5993s
INFO:root:Epoch: 0080 val_loss: 1.4825 val_roc: 0.7279 val_ap: 0.6829
INFO:root:Epoch: 0085 lr: 0.01 train_loss: 1.1936 train_roc: 0.8251 train_ap: 0.7737 time: 0.5835s
INFO:root:Epoch: 0085 val_loss: 1.4731 val_roc: 0.7305 val_ap: 0.6851
INFO:root:Epoch: 0090 lr: 0.01 train_loss: 1.2167 train_roc: 0.8404 train_ap: 0.8044 time: 0.5947s
INFO:root:Epoch: 0090 val_loss: 1.4669 val_roc: 0.7237 val_ap: 0.6821
INFO:root:Epoch: 0095 lr: 0.01 train_loss: 1.7581 train_roc: 0.6856 train_ap: 0.6909 time: 0.6168s
INFO:root:Epoch: 0095 val_loss: 1.4928 val_roc: 0.7152 val_ap: 0.6652
INFO:root:Epoch: 0100 lr: 0.01 train_loss: 1.2111 train_roc: 0.8177 train_ap: 0.7633 time: 0.5971s
INFO:root:Epoch: 0100 val_loss: 1.4424 val_roc: 0.7367 val_ap: 0.6992
INFO:root:Epoch: 0105 lr: 0.01 train_loss: 1.1804 train_roc: 0.8582 train_ap: 0.8230 time: 0.6288s
INFO:root:Epoch: 0105 val_loss: 1.4458 val_roc: 0.7399 val_ap: 0.7037
INFO:root:Epoch: 0110 lr: 0.01 train_loss: 1.1671 train_roc: 0.8432 train_ap: 0.8035 time: 0.5899s
INFO:root:Epoch: 0110 val_loss: 1.4345 val_roc: 0.7383 val_ap: 0.6976
INFO:root:Early stopping
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 105.9660s
INFO:root:Val set results: val_loss: 2.2535 val_roc: 0.8549 val_ap: 0.8580
INFO:root:Test set results: test_loss: 2.2535 test_roc: 0.8380 test_ap: 0.8485
INFO:root:Saved model in /workspace/xiongbo/hgcn/logs/lp/2021_4_22/18
